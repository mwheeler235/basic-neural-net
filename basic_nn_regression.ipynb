{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de77fbc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec53a966",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/walmart_sales/Walmart_Sales.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c8ce484",
   "metadata": {},
   "source": [
    "#### Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e881e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('Store')['Weekly_Sales'].mean().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356a0a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use .copy() to avoid SettingWithCopyWarning\n",
    "# Subset the DataFrame for Store 4 only\n",
    "df_store_4 = df[df['Store'] == 4].copy()\n",
    "df_store_4['Date'] = pd.to_datetime(df_store_4['Date'], dayfirst=True)\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(df_store_4['Date'], df_store_4['Weekly_Sales'])\n",
    "plt.title('Weekly Sales Over Time, Store 4')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Weekly Sales')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e834df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar chart of average weekly sales by week of year (52 weeks)\n",
    "df_store_4['weekofyear'] = df_store_4['Date'].dt.isocalendar().week\n",
    "avg_sales_by_week = df_store_4.groupby('weekofyear')['Weekly_Sales'].mean()\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.bar(avg_sales_by_week.index, avg_sales_by_week.values)\n",
    "plt.xlabel('Week of Year')\n",
    "plt.ylabel('Average Weekly Sales')\n",
    "plt.title('Average Weekly Sales by Week of Year (Store 4)')\n",
    "plt.xticks(range(1, 53))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16522be0",
   "metadata": {},
   "source": [
    "#### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead9f251",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add week of year and lagged sales features\n",
    "store4_fe = df_store_4.copy()\n",
    "store4_fe['weekofyear'] = store4_fe['Date'].dt.isocalendar().week\n",
    "store4_fe['lag1'] = store4_fe['Weekly_Sales'].shift(1)\n",
    "store4_fe['lag2'] = store4_fe['Weekly_Sales'].shift(2)\n",
    "# Drop rows with NaN (from lagging)\n",
    "store4_fe = store4_fe.dropna().reset_index(drop=True)\n",
    "store4_fe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e741b4",
   "metadata": {},
   "source": [
    "#### Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9228e3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "holdout_size = 10   # number of weeks in holdout set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b6ef65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare new feature matrix and target for the new model\n",
    "train_fe = store4_fe.iloc[:-holdout_size]\n",
    "test_fe = store4_fe.iloc[-holdout_size:]\n",
    "\n",
    "# Features: Date (ordinal), weekofyear, lag1, lag2\n",
    "X_train_fe = np.column_stack([\n",
    "    train_fe['Date'].map(lambda d: d.toordinal()).values,\n",
    "    train_fe['weekofyear'].values,\n",
    "    train_fe['lag1'].values,\n",
    "    train_fe['lag2'].values\n",
    "])\n",
    "y_train_fe = train_fe['Weekly_Sales'].values.reshape(-1, 1)\n",
    "X_test_fe = np.column_stack([\n",
    "    test_fe['Date'].map(lambda d: d.toordinal()).values,\n",
    "    test_fe['weekofyear'].values,\n",
    "    test_fe['lag1'].values,\n",
    "    test_fe['lag2'].values\n",
    "])\n",
    "y_test_fe = test_fe['Weekly_Sales'].values.reshape(-1, 1)\n",
    "\n",
    "# Scale features and target\n",
    "x_scaler_fe = MinMaxScaler()\n",
    "y_scaler_fe = MinMaxScaler()\n",
    "X_train_fe_scaled = x_scaler_fe.fit_transform(X_train_fe)\n",
    "y_train_fe_scaled = y_scaler_fe.fit_transform(y_train_fe)\n",
    "X_test_fe_scaled = x_scaler_fe.transform(X_test_fe)\n",
    "y_test_fe_scaled = y_scaler_fe.transform(y_test_fe)\n",
    "\n",
    "print('X_train_fe_scaled shape:', X_train_fe_scaled.shape)\n",
    "print('y_train_fe_scaled shape:', y_train_fe_scaled.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca61b79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize X and y for training\n",
    "train_df = df_store_4.iloc[:-holdout_size]\n",
    "test_df = df_store_4.iloc[-holdout_size:]\n",
    "\n",
    "# Prepare X and y\n",
    "X_train = train_df['Date'].map(lambda d: d.toordinal()).values.reshape(-1, 1)\n",
    "y_train = train_df['Weekly_Sales'].values.reshape(-1, 1)\n",
    "X_test = test_df['Date'].map(lambda d: d.toordinal()).values.reshape(-1, 1)\n",
    "y_test = test_df['Weekly_Sales'].values.reshape(-1, 1)\n",
    "\n",
    "# Fit scalers on training data only\n",
    "x_scaler = MinMaxScaler()\n",
    "y_scaler = MinMaxScaler()\n",
    "X_train_scaled = x_scaler.fit_transform(X_train)\n",
    "y_train_scaled = y_scaler.fit_transform(y_train)\n",
    "X_test_scaled = x_scaler.transform(X_test)\n",
    "y_test_scaled = y_scaler.transform(y_test)\n",
    "\n",
    "print('X_train_scaled shape:', X_train_scaled.shape)\n",
    "print('y_train_scaled shape:', y_train_scaled.shape)\n",
    "\n",
    "print('X_train_scaled range:', X_train_scaled.min(), X_train_scaled.max())\n",
    "print('y_train_scaled range:', y_train_scaled.min(), y_train_scaled.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e121a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ReLU activation function\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "# Derivative of ReLU activation\n",
    "def relu_deriv(x):\n",
    "    return (x > 0).astype(float)\n",
    "\n",
    "# Mean Squared Error loss function\n",
    "def mse_loss(y_true, y_pred):\n",
    "    return np.mean((y_true - y_pred) ** 2)\n",
    "\n",
    "# Derivative of Mean Squared Error loss\n",
    "def mse_loss_deriv(y_true, y_pred):\n",
    "    return 2 * (y_pred - y_true) / y_true.size\n",
    "\n",
    "# Simple feedforward neural network with one hidden layer and L2 regularization\n",
    "class SimpleNeuralNetwork:\n",
    "    def __init__(self, input_size, hidden_size, output_size, lambda_l2=0.0):\n",
    "        self.W1 = np.random.randn(input_size, hidden_size)\n",
    "        self.b1 = np.zeros((1, hidden_size))\n",
    "        self.W2 = np.random.randn(hidden_size, output_size)\n",
    "        self.b2 = np.zeros((1, output_size))\n",
    "        self.lambda_l2 = lambda_l2\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.z1 = X @ self.W1 + self.b1\n",
    "        self.a1 = relu(self.z1)\n",
    "        self.z2 = self.a1 @ self.W2 + self.b2\n",
    "        return self.z2\n",
    "\n",
    "    def backward(self, X, y, y_pred, lr=0.001):\n",
    "        dz2 = mse_loss_deriv(y, y_pred)\n",
    "        dW2 = self.a1.T @ dz2 + self.lambda_l2 * self.W2\n",
    "        db2 = np.sum(dz2, axis=0, keepdims=True)\n",
    "        da1 = dz2 @ self.W2.T\n",
    "        dz1 = da1 * relu_deriv(self.z1)\n",
    "        dW1 = X.T @ dz1 + self.lambda_l2 * self.W1\n",
    "        db1 = np.sum(dz1, axis=0, keepdims=True)\n",
    "        self.W2 -= lr * dW2\n",
    "        self.b2 -= lr * db2\n",
    "        self.W1 -= lr * dW1\n",
    "        self.b1 -= lr * db1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7651c62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "l2_lambda = 0.01  # L2 regularization strength, to prevent overfitting\n",
    "hidden_size = 6   # Number of hidden units (reduced to also prevent overfitting)\n",
    "epochs = 1000 # Number of training epochs\n",
    "lr = 0.01 # Learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aacd9517",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the 'no feature engineering' model is trained and used with 1 feature\n",
    "nn = SimpleNeuralNetwork(input_size=1, hidden_size=hidden_size, output_size=1, lambda_l2=l2_lambda)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    y_pred = nn.forward(X_train_scaled)\n",
    "    loss = mse_loss(y_train_scaled, y_pred)\n",
    "    nn.backward(X_train_scaled, y_train_scaled, y_pred, lr)\n",
    "    if (epoch + 1) % 200 == 0:\n",
    "        print(f\"[No FE Model] Epoch {epoch+1}, Loss: {loss:.4f}\")\n",
    "\n",
    "# Generate predictions for the holdout set (no feature engineering)\n",
    "y_test_pred_scaled = nn.forward(X_test_scaled)\n",
    "y_test_pred = y_scaler.inverse_transform(y_test_pred_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc92c2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_fe = SimpleNeuralNetwork(input_size=4, hidden_size=hidden_size, output_size=1, lambda_l2=l2_lambda)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    y_pred = nn_fe.forward(X_train_fe_scaled)\n",
    "    loss = mse_loss(y_train_fe_scaled, y_pred)\n",
    "    nn_fe.backward(X_train_fe_scaled, y_train_fe_scaled, y_pred, lr)\n",
    "    if (epoch + 1) % 200 == 0:\n",
    "        print(f\"[FE Model] Epoch {epoch+1}, Loss: {loss:.4f}\")\n",
    "\n",
    "# Evaluate the new model's predictions and metrics on the holdout set\n",
    "y_test_pred_fe_scaled = nn_fe.forward(X_test_fe_scaled)\n",
    "y_test_pred_fe = y_scaler_fe.inverse_transform(y_test_pred_fe_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38de211c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate RMSE, RMSE/Mean, and MAPE for the holdout set predictions (no feature engineering)\n",
    "rmse = mean_squared_error(y_test.flatten(), y_test_pred.flatten())\n",
    "mean_actual = np.mean(y_test)\n",
    "rmse_over_mean = rmse / mean_actual\n",
    "mape = np.mean(np.abs((y_test.flatten() - y_test_pred.flatten()) / y_test.flatten())) * 100\n",
    "print(f\"[No FE] RMSE: {rmse:.2f}\")\n",
    "print(f\"[No FE] RMSE / Mean Actual: {rmse_over_mean:.4f}\")\n",
    "print(f\"[No FE] MAPE: {mape:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49acb5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics\n",
    "rmse_fe = mean_squared_error(y_test_fe.flatten(), y_test_pred_fe.flatten())\n",
    "mean_actual_fe = np.mean(y_test_fe)\n",
    "rmse_over_mean_fe = rmse_fe / mean_actual_fe\n",
    "mape_fe = np.mean(np.abs((y_test_fe.flatten() - y_test_pred_fe.flatten()) / y_test_fe.flatten())) * 100\n",
    "print(f\"[FE Model] RMSE: {rmse_fe:.2f}\")\n",
    "print(f\"[FE Model] RMSE / Mean Actual: {rmse_over_mean_fe:.4f}\")\n",
    "print(f\"[FE Model] MAPE: {mape_fe:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df56149b",
   "metadata": {},
   "source": [
    "### Model Comparison: With and Without Feature Engineering\n",
    "This section compares the performance of two neural network models:\n",
    "- **Model 1:** Uses only the date as a feature (no feature engineering)\n",
    "- **Model 2:** Uses engineered features (date, week of year, lagged sales)\n",
    "Metrics and plots for both models are shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6412202f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize metrics for both models (no FE vs. FE)\n",
    "print('Model Comparison (Holdout Set):')\n",
    "print('-' * 40)\n",
    "print(f\"No Feature Engineering:\\n  RMSE: {rmse:.2f}\\n  RMSE/Mean: {rmse_over_mean:.4f}\\n  MAPE: {mape:.2f}%\\n\")\n",
    "print(f\"With Feature Engineering:\\n  RMSE: {rmse_fe:.2f}\\n  RMSE/Mean: {rmse_over_mean_fe:.4f}\\n  MAPE: {mape_fe:.2f}%\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4786a743",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot both models' predictions vs. actuals for the holdout set\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(test_df['Date'], y_test.flatten(), label='Actual', marker='o')\n",
    "plt.plot(test_df['Date'], y_test_pred.flatten(), label='Predicted (No FE)', marker='x')\n",
    "plt.plot(test_fe['Date'], y_test_pred_fe.flatten(), label='Predicted (With FE)', marker='s')\n",
    "plt.title('Actual vs. Predicted Weekly Sales (Holdout Set), Store 4')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Weekly Sales')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
